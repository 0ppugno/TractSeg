#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2017 Division of Medical Image Computing, German Cancer Research Center (DKFZ)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from os.path import join
from collections import defaultdict
import argparse

import nibabel as nib
import numpy as np
from nibabel import trackvis
from scipy.ndimage.morphology import binary_dilation
from dipy.tracking.streamline import transform_streamlines
from scipy.ndimage.interpolation import map_coordinates
from dipy.tracking.streamline import values_from_volume
from dipy.tracking.streamline import Streamlines
from dipy.segment.clustering import QuickBundles
from dipy.segment.metric import AveragePointwiseEuclideanMetric
from scipy.spatial import cKDTree
import dipy.stats.analysis as dsa
from dipy.tracking.streamline import Streamlines

from tractseg.libs import fiber_utils
from tractseg.libs import exp_utils


def get_length_best_orig_peak(predicted_img, orig_img, x, y, z):
    predicted = predicted_img[x, y, z, :]       # 1 peak
    orig = [orig_img[x, y, z, 0:3], orig_img[x, y, z, 3:6], orig_img[x, y, z, 6:9]]     # 3 peaks

    angle1 = abs(np.dot(predicted, orig[0]) / (np.linalg.norm(predicted) * np.linalg.norm(orig[0]) + 1e-7))
    angle2 = abs(np.dot(predicted, orig[1]) / (np.linalg.norm(predicted) * np.linalg.norm(orig[1]) + 1e-7))
    angle3 = abs(np.dot(predicted, orig[2]) / (np.linalg.norm(predicted) * np.linalg.norm(orig[2]) + 1e-7))

    argmax = np.argmax([angle1, angle2, angle3])
    best_peak_len = np.linalg.norm(orig[argmax])
    return best_peak_len


def orient_to_same_start_region(streamlines, beginnings):
    # (we could also use dipy.tracking.streamline.orient_by_streamline instead)
    streamlines_new = []
    for idx, sl in enumerate(streamlines):
        startpoint = sl[0]
        # Flip streamline if not in right order
        if beginnings[int(startpoint[0]), int(startpoint[1]), int(startpoint[2])] == 0:
            sl = sl[::-1, :]
        streamlines_new.append(sl)
    return streamlines_new


def evaluate_along_streamlines(scalar_img, streamlines, beginnings, nr_points, dilate=0, predicted_peaks=None,
                               affine=None):
    # Runtime:
    # - default:                2.7s (test),    56s (all),      10s (test 4 bundles, 100 points)
    # - map_coordinate order 1: 1.9s (test),    26s (all),       6s (test 4 bundles, 100 points)
    # - map_coordinate order 3: 2.2s (test),    33s (all),
    # - values_from_volume:     2.5s (test),    43s (all),
    # - AFQ:                      ?s (test),     ?s (all),      85s  (test 4 bundles, 100 points)
    # => AFQ a lot slower than others

    for i in range(dilate):
        beginnings = binary_dilation(beginnings)
    beginnings = beginnings.astype(np.uint8)

    # THIS IS ONLY NEEDED FOR "MANUAL":
    # Move from convention "0mm is in voxel center" to convention "0mm is in voxel corner". This makes it easier
    # to calculate the voxel a streamline point is located in.
    # (dipy is working with the convention "0mm is in voxel center" therefore this is not needed there)
    # print("INFO: Adding 0.5 to streamlines")
    # streamlines = fiber_utils.add_to_each_streamline(streamlines, 0.5)

    streamlines = orient_to_same_start_region(streamlines, beginnings)

    ### Sampling ###

    #################################### Sampling "MANUAL" ############################
    # values = []
    # for i in range(nr_points):
    #     values.append([])
    #
    # for idx, sl in enumerate(streamlines):
    #     for jdx in range(sl.shape[0]):   #iterate over nr_points
    #         point = sl[jdx]
    #         if predicted_peaks is not None:
    #             scalar_value = get_length_best_orig_peak(predicted_peaks, scalar_img,
    #                                                      int(point[0]), int(point[1]), int(point[2]))
    #         else:
    #             scalar_value = scalar_img[int(point[0]), int(point[1]), int(point[2])]
    #         values[jdx].append(scalar_value)
    ###################################################################################

    #################################### Sampling map_coordinates #####################
    values = map_coordinates(scalar_img, np.array(streamlines).T, order=1)
    ###################################################################################

    #################################### Sampling values_from_volume ##################
    # streamlines = list(transform_streamlines(streamlines, affine))  # this has to be here; not remove previous one
    # values = np.array(values_from_volume(scalar_img, streamlines, affine=affine)).T
    ###################################################################################


    ### Aggregation ###

    #################################### Aggregating by MEAN ##########################
    # values_mean = np.array(values).mean(axis=1)
    # values_std = np.array(values).std(axis=1)
    # return values_mean, values_std
    ###################################################################################

    #################################### Aggregating by cKDTree #######################
    metric = AveragePointwiseEuclideanMetric()
    qb = QuickBundles(threshold=100., metric=metric)
    clusters = qb.cluster(streamlines)
    centroids = Streamlines(clusters.centroids)
    if len(centroids) > 1:
        print("WARNING: number clusters > 1 ({})".format(len(centroids)))
    _, segment_idxs = cKDTree(centroids.data, 1, copy_data=True).query(streamlines, k=1)  # (2000, 20)

    values_t = np.array(values).T  # (2000, 20)

    # If we want to take weighted mean like in AFQ:
    # weights = dsa.gaussian_weights(Streamlines(streamlines))
    # values_t = weights * values_t
    # return np.sum(values_t, 0), None

    results_dict = defaultdict(list)
    for idx, sl in enumerate(values_t):
        for jdx, seg in enumerate(sl):
            results_dict[segment_idxs[idx, jdx]].append(seg)

    if len(results_dict.keys()) < nr_points:
        print("WARNING: found less than required points. Filling up with centroid values.")
        centroid_values = map_coordinates(scalar_img, np.array([centroids[0]]).T, order=1)
        for i in range(nr_points):
            if len(results_dict[i]) == 0:
                results_dict[i].append(np.array(centroid_values).T[0, i])

    results_mean = []
    results_std = []
    for key in sorted(results_dict.keys()):
        value = results_dict[key]
        if len(value) > 0:
            results_mean.append(np.array(value).mean())
            results_std.append(np.array(value).std())
        else:
            print("WARNING: empty segment")
            results_mean.append(0)
            results_std.append(0)

    return results_mean, results_std
    ###################################################################################

    #################################### AFQ (sampling + aggregation ##################
    # streamlines = list(transform_streamlines(streamlines, affine))  # this has to be here; not remove previous one
    # streamlines = Streamlines(streamlines)
    # weights = dsa.gaussian_weights(streamlines)
    # results_mean = dsa.afq_profile(scalar_img, streamlines, affine=affine, weights=weights)
    # results_std = None
    # return results_mean, results_std
    ###################################################################################



def main():
    parser = argparse.ArgumentParser(description="Evaluate image (e.g. FA) along fiber bundles.",
                                     epilog="Written by Jakob Wasserthal. Please reference 'Wasserthal et al. "
                                            "TractSeg - Fast and accurate white matter tract segmentation. "
                                            "https://doi.org/10.1016/j.neuroimage.2018.07.070)'")
    parser.add_argument("-i", metavar="tracking_dir", dest="tracking_dir",
                        help="Folder containing the TractSeg tractograms (normally '.../tractseg_output/TOM_tracking')",
                        required=True)
    parser.add_argument("-o", metavar="csv_output", dest="csv_file_out",
                        help="CSV output file containing the results", required=True)
    parser.add_argument("-e", metavar="endings_dir", dest="endings_dir",
                        help="Folder containing the TractSeg bundle endings segmentations "
                             "(normally '.../tractseg_output/endings_segmentations'). "
                             "Needed to ensure that all fibers are starting from the same side.", required=True)
    parser.add_argument("-s", metavar="scalar_img", dest="scalar_img",
                        help="Scalar image (e.g. FA) or peak image (MRtrix peaks) if using '--peak_length'",
                        required=True)
    parser.add_argument("--nr_points", metavar="n", dest="nr_points",
                        help="Number of points along streamline to evaluate (default: 100)", default="100")
    parser.add_argument("--peak_length", action="store_true",
                        help="Instead of taking values of scalar image along streamlines (e.g. FA) take the length "
                             "of the peak pointing in the same direction as the respective bundle. Gives better "
                             "results in areas of crossing fibers than FA.",
                        default=False)
    parser.add_argument("--TOM", metavar="TOM_dir", dest="TOM_dir",
                        help="Folder containing Tract Orientation Maps (TOMs) (normally '.../tractseg_output/TOM'). "
                             "Needed if using the '--peak_length' option.")
    parser.add_argument("--tracking_format", metavar="tck|trk|trk_legacy", choices=["tck", "trk", "trk_legacy"],
                        help="Set output format of tracking. For trk also the option trk_legacy is available. This "
                             "uses the older trk convention (streamlines are stored in coordinate space and affine is "
                             "not applied. See nibabel.trackvis.read. (default: trk_legacy)",
                        default="trk_legacy")
    parser.add_argument("--test", metavar="0|1|2", choices=[0, 1, 2], type=int,
                        help="Only needed for unittesting.",
                        default=0)
    args = parser.parse_args()

    NR_POINTS = int(args.nr_points)
    # Dilation >0 important because otherwise some streamlines do not start/end in beginnings region and then
    # correct reorientation/flipping of streamlines does not work anymore
    DILATION = 2
    scalar_image = nib.load(args.scalar_img)

    if args.test == 1:
        bundles = exp_utils.get_bundle_names("test")[1:]
    elif args.test == 2:
        bundles = exp_utils.get_bundle_names("toy")[1:]
        DILATION = 0
    else:
        bundles = exp_utils.get_bundle_names("All_tractometry")[1:]

    results = []
    for bundle in bundles:
        if args.peak_length:
            predicted_peaks = nib.load(join(args.TOM_dir, bundle + ".nii.gz")).get_data()
        else:
            predicted_peaks = None
        beginnings = nib.load(join(args.endings_dir, bundle + "_b.nii.gz"))

        if args.tracking_format == "trk_legacy":
            streams, hdr = trackvis.read(join(args.tracking_dir, bundle + ".trk"))
            streamlines = [s[0] for s in streams]
        else:
            sl_file = nib.streamlines.load(join(args.tracking_dir, bundle + "." + args.tracking_format))
            streamlines = sl_file.streamlines

        if len(streamlines) > 0:
            streamlines = fiber_utils.resample_fibers(streamlines, nb_points=NR_POINTS)    #mean over bundles
            streamlines = list(transform_streamlines(streamlines, np.linalg.inv(scalar_image.affine)))
            mean, std = evaluate_along_streamlines(np.nan_to_num(scalar_image.get_data()), streamlines,
                                                   beginnings.get_data(), NR_POINTS, dilate=DILATION,
                                                   predicted_peaks=predicted_peaks, affine=scalar_image.affine)
        else:
            print("WARNING: bundle {} contains no streamlines. Saving value 0 for this bundle.".format(bundle))
            mean = np.zeros(NR_POINTS)
            std = np.zeros(NR_POINTS)
        results.append(mean)

    bundle_string = ""
    for bundle in bundles:
        bundle_string += bundle + ";"
    bundle_string = bundle_string[:-1]

    np.savetxt(args.csv_file_out, np.array(results).transpose(), delimiter=";", header=bundle_string, comments="")

    # Notes on reproducibility
    # - map_coordinates, QuickBundles and cKDTree are deterministic for the same input streamlines
    # - Variance in final Tractometry results when running 2 probabilistic trackings (10k fibers and 100 points):
    #   - Max difference ~0.01, but on average difference around 0.004
    #   - When plotting the differences: very minor; might get averaged out in group analysis


if __name__ == '__main__':
    main()