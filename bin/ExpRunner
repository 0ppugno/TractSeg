#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2017 Division of Medical Image Computing, German Cancer Research Center (DKFZ)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import warnings
warnings.simplefilter("ignore", UserWarning)    #hide scipy warnings
warnings.simplefilter("ignore", FutureWarning)    #hide h5py warnings

import os
import importlib
import argparse
import pickle
import time
from pprint import pprint
import distutils.util
from os.path import join
import nibabel as nib
import numpy as np

from tractseg.libs.DirectionMerger import DirectionMerger
from tractseg.libs.ExpUtils import ExpUtils
from tractseg.libs.ImgUtils import ImgUtils
from tractseg.libs.MetricUtils import MetricUtils
from tractseg.libs.ClusterUtils import ClusterUtils
from tractseg.libs.Config import Config as C
from tractseg.libs.Trainer import Trainer
from tractseg.libs.DataManagers import DataManagerSingleSubjectById
from tractseg.libs.DataManagers import DataManagerTrainingNiftiImgs
from tractseg.libs.DataManagers import DataManagerPrecomputedBatches
from tractseg.libs.DataManagers import DataManagerPrecomputedBatches_noDLBG
from tractseg.models.BaseModel import BaseModel

'''
#Run ubuntu:
ExpRunner --config=XXX

#Run cluster:
sbatch --job-name=XXX ~/runner.sh

#Predicting with new config setup:
ExpRunner --train=False --seg --lw --config=XXX
ExpRunner --train=False --test=True --lw --config=XXX
'''

parser = argparse.ArgumentParser(description="Train a network on your own data to segment white matter bundles.",
                                    epilog="Written by Jakob Wasserthal. Please reference 'Wasserthal et al. " +
                                           "TractSeg - Fast and accurate white matter tract segmentation. https://arxiv.org/abs/1805.07103)'")
parser.add_argument("--config", metavar="name", help="Name of configuration to use")
parser.add_argument("--train", metavar="True/False", help="Train network", type=distutils.util.strtobool, default=True)
parser.add_argument("--test", metavar="True/False", help="Test network", type=distutils.util.strtobool, default=False)
parser.add_argument("--seg", action="store_true", help="Create binary segmentation")
parser.add_argument("--probs", action="store_true", help="Create probmap segmentation")
parser.add_argument("--lw", action="store_true", help="Load weights of pretrained net")
parser.add_argument("--en", metavar="name", help="Experiment name")
parser.add_argument("--fold", metavar="N", help="Which fold to train when doing CrossValidation", type=int)
parser.add_argument("--verbose", action="store_true", help="Show more intermediate output", default=True)   #set default to False in future
parser.add_argument('--version', action='version', version='TractSeg 1.3')
args = parser.parse_args()

HP = getattr(importlib.import_module("tractseg.config.BaseHP"), "HP")()
if args.config:
    HP = getattr(importlib.import_module("tractseg.config.custom." + args.config), "HP")()    #HP.__dict__ does not work if ()
    # HP.EXP_NAME = args.config

if args.en:
    HP.EXP_NAME = args.en

HP.TRAIN = bool(args.train)
HP.TEST = bool(args.test)
if args.seg:
    HP.SEGMENT = True
if args.probs:
    HP.GET_PROBS = True
if args.lw:
    HP.LOAD_WEIGHTS = args.lw
if args.fold:
    HP.CV_FOLD= args.fold
HP.VERBOSE = args.verbose

# if os.environ.get("TRACTSEG_USE_CLUSTER") is not None:
#     tmp_ssd_folder = ClusterUtils.copy_training_files_to_ssd(HP, join(C.HOME, HP.DATASET_FOLDER))

HP.MULTI_PARENT_PATH = join(C.EXP_PATH, HP.EXP_MULTI_NAME)
HP.EXP_PATH = join(C.EXP_PATH, HP.EXP_MULTI_NAME, HP.EXP_NAME)
HP.TRAIN_SUBJECTS, HP.VALIDATE_SUBJECTS, HP.TEST_SUBJECTS = ExpUtils.get_cv_fold(HP.CV_FOLD, dataset=HP.DATASET)

if HP.WEIGHTS_PATH == "":
    HP.WEIGHTS_PATH = ExpUtils.get_best_weights_path(HP.EXP_PATH, HP.LOAD_WEIGHTS)

#Autoset some parameters based on settings
if HP.RESOLUTION == "1.25mm":
    HP.INPUT_DIM = (144, 144)
elif HP.RESOLUTION == "2mm":
    HP.INPUT_DIM = (96, 96)
elif HP.RESOLUTION == "2.5mm":
    HP.INPUT_DIM = (80, 80)
HP = ExpUtils.get_labels_filename(HP)

if HP.EXPERIMENT_TYPE == "peak_regression":
    HP.NR_OF_CLASSES = 3*len(ExpUtils.get_bundle_names(HP.CLASSES)[1:])
else:
    HP.NR_OF_CLASSES = len(ExpUtils.get_bundle_names(HP.CLASSES)[1:])

if HP.TRAIN:
    HP.EXP_PATH = ExpUtils.create_experiment_folder(HP.EXP_NAME, HP.MULTI_PARENT_PATH, HP.TRAIN)

if HP.VERBOSE:
    print("Hyperparameters:")
    ExpUtils.print_HPs(HP)

with open(join(HP.EXP_PATH, "Hyperparameters.txt"), "w") as f:
    HP_dict = {attr: getattr(HP, attr) for attr in dir(HP) if not callable(getattr(HP, attr)) and not attr.startswith("__")}
    pprint(HP_dict, f)

def test_whole_subject(HP, model, subjects, type):

    metrics = {
        "loss_" + type: [0],
        "f1_macro_" + type: [0],
    }

    # Metrics per bundle
    metrics_bundles = {}
    for bundle in ExpUtils.get_bundle_names(HP.CLASSES)[1:]:
        metrics_bundles[bundle] = [0]

    for subject in subjects:
        print("{} subject {}".format(type, subject))
        start_time = time.time()

        dataManagerSingle = DataManagerSingleSubjectById(HP, subject=subject)
        trainerSingle = Trainer(model, dataManagerSingle)
        img_probs, img_y = trainerSingle.get_seg_single_img(HP, probs=True)
        # img_probs_xyz, img_y = DirectionMerger.get_seg_single_img_3_directions(HP, model, subject=subject)
        # img_probs = DirectionMerger.mean_fusion(HP.THRESHOLD, img_probs_xyz, probs=True)

        print("Took {}s".format(round(time.time() - start_time, 2)))

        if HP.EXPERIMENT_TYPE == "peak_regression":
            f1 = MetricUtils.calc_peak_length_dice(HP, img_probs, img_y, max_angle_error=HP.PEAK_DICE_THR, max_length_error=HP.PEAK_DICE_LEN_THR)
            peak_f1_mean = np.array([s for s in f1.values()]).mean()  # if f1 for multiple bundles
            metrics = MetricUtils.calculate_metrics(metrics, None, None, 0, f1=peak_f1_mean, type=type, threshold=HP.THRESHOLD)
            metrics_bundles = MetricUtils.calculate_metrics_each_bundle(metrics_bundles, None, None, ExpUtils.get_bundle_names(HP.CLASSES)[1:], f1, threshold=HP.THRESHOLD)
        else:
            img_probs = np.reshape(img_probs, (-1, img_probs.shape[-1]))  #Flatten all dims except nrClasses dim
            img_y = np.reshape(img_y, (-1, img_y.shape[-1]))
            metrics = MetricUtils.calculate_metrics(metrics, img_y, img_probs, 0, type=type, threshold=HP.THRESHOLD)
            metrics_bundles = MetricUtils.calculate_metrics_each_bundle(metrics_bundles, img_y, img_probs, ExpUtils.get_bundle_names(HP.CLASSES)[1:], threshold=HP.THRESHOLD)

    metrics = MetricUtils.normalize_last_element(metrics, len(subjects), type=type)
    metrics_bundles = MetricUtils.normalize_last_element_general(metrics_bundles, len(subjects))

    print("WHOLE SUBJECT:")
    pprint(metrics)
    print("WHOLE SUBJECT BUNDLES:")
    pprint(metrics_bundles)


    with open(join(HP.EXP_PATH, "score_" + type + "-set.txt"), "w") as f:
        pprint(metrics, f)
        f.write("\n\nWeights: {}\n".format(HP.WEIGHTS_PATH))
        f.write("type: {}\n\n".format(type))
        pprint(metrics_bundles, f)

    pickle.dump(metrics, open(join(HP.EXP_PATH, "score_" + type + ".pkl"), "wb"))

    return metrics


dataManager = DataManagerTrainingNiftiImgs(HP)
# dataManager = DataManagerPrecomputedBatches(HP)
# dataManager = DataManagerPrecomputedBatches_noDLBG(HP)

# ModelClass = getattr(importlib.import_module("tractseg.models." + HP.MODEL), HP.MODEL)
# model = ModelClass(HP)
model = BaseModel(HP)
trainer = Trainer(model, dataManager)

if HP.TRAIN:
    print("Training...")
    metrics = trainer.train(HP)

#After Training
if HP.TRAIN:
    # have to load other weights, because after training it has the weights of the last epoch
    print("Loading best epoch: {}".format(HP.BEST_EPOCH))
    HP.WEIGHTS_PATH = HP.EXP_PATH + "/best_weights_ep" + str(HP.BEST_EPOCH) + ".npz"
    HP.LOAD_WEIGHTS = True
    trainer.model.load_model(join(HP.EXP_PATH, HP.WEIGHTS_PATH))
    model_test = trainer.model
else:
    # Weight_path already set to best model (wenn reading program parameters) -> will be loaded automatically
    model_test = trainer.model

if HP.SEGMENT:
    ExpUtils.make_dir(join(HP.EXP_PATH, "segmentations"))
    # all_subjects = HP.VALIDATE_SUBJECTS #+ HP.TEST_SUBJECTS
    all_subjects = ["599469"]
    for subject in all_subjects:
        print("Get_segmentation subject {}".format(subject))
        start_time = time.time()

        if HP.EXPERIMENT_TYPE == "peak_regression":
            dataManagerSingle = DataManagerSingleSubjectById(HP, subject=subject, use_gt_mask=False)
            trainerSingle = Trainer(model_test, dataManagerSingle)
            img_probs, img_y = trainerSingle.get_seg_single_img(HP, probs=True)  # only x or y or z
            img_seg = ImgUtils.peak_image_to_binary_mask(img_probs, len_thr=0.4)  # thr: 0.4 slightly better than 0.2
        else:
            img_seg, img_y = DirectionMerger.get_seg_single_img_3_directions(HP, model, subject)  #returns probs not binary seg
            img_seg = DirectionMerger.mean_fusion(HP.THRESHOLD, img_seg, probs=False)

        #TractSeg
        # ImgUtils.save_multilabel_img_as_multiple_files(HP, img_seg, ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION),
        #                                                      join(HP.EXP_PATH, "segmentations_" + subject))

        #Tract Beginnings and Endings
        # ImgUtils.save_multilabel_img_as_multiple_files_endings(HP, img_seg, ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION),
        #                                                        join(HP.EXP_PATH, "segmentations_" + subject))

        img = nib.Nifti1Image(img_seg.astype(np.uint8), ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION))
        nib.save(img, join(HP.EXP_PATH, "segmentations", subject + "_segmentation.nii.gz"))
        # print("took {}s".format(time.time() - start_time))

if HP.TEST:
    test_whole_subject(HP, model_test, HP.VALIDATE_SUBJECTS, "validate")
    # test_whole_subject(HP, model_test, HP.TEST_SUBJECTS, "test")

if HP.GET_PROBS:
    ExpUtils.make_dir(join(HP.EXP_PATH, "probmaps"))
    # all_subjects = HP.TEST_SUBJECTS
    all_subjects = ["599469", "992774", "994273"]
    # all_subjects = HP.TRAIN_SUBJECTS + HP.VALIDATE_SUBJECTS + HP.TEST_SUBJECTS
    for subject in all_subjects:
        print("Get_probs subject {}".format(subject))

        dataManagerSingle = DataManagerSingleSubjectById(HP, subject=subject, use_gt_mask=False)

        if HP.EXPERIMENT_TYPE == "peak_regression":
            trainerSingle = Trainer(model_test, dataManagerSingle)
            img_probs, img_y = trainerSingle.get_seg_single_img(HP, probs=True)
            img_probs = ImgUtils.remove_small_peaks(img_probs, len_thr=0.4)
        else:
            img_probs, img_y = DirectionMerger.get_seg_single_img_3_directions(HP, model, subject=subject)
            img_probs = DirectionMerger.mean_fusion(HP.THRESHOLD, img_probs, probs=True)

        # ImgUtils.save_multilabel_img_as_multiple_files_peaks(HP, img_probs, ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION),
        #                                                      join(HP.EXP_PATH, "probmaps_" + subject))

        # ImgUtils.save_multilabel_img_as_multiple_files(HP, img_probs, ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION),
        #                                                      join(HP.EXP_PATH, "probmaps_" + subject))

        img = nib.Nifti1Image(img_probs, ImgUtils.get_dwi_affine(HP.DATASET, HP.RESOLUTION))
        nib.save(img, join(HP.EXP_PATH, "probmaps", subject + "_peak.nii.gz"))

